\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsfonts, mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx, lscape, forloop, url, natbib, blindtext}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}

\hypersetup{colorlinks=true,urlcolor=black}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\st}[1]{{\color{red} #1}}

\begin{document}
\lhead{Amazon Turk Experiment 1}
\chead{}
\rhead{Samantha Tyner}

\tableofcontents

\section{Introduction}

\st{XXX Most of this I copied over from the other paper so it might not be super coherent right now. XXX}
% ALTERNATE OPENER: Stochastic actor-oriented models (SAOMs) are used to model dynamic social networks. In these models, there are three different sets of parameters: rate, structural, and covariate. The rate parameters dictate how often the actors in the network change ties; the structural parameters dictate how ties change based solely on the network structure; the covariate parameters dictate how ties change based on actor characteristics. The many parameters in the model 

% UNDERLYING THEORY
Stochastic actor-oriented models (SAOMs) are a set of network models that consider both network and actor effects when determining how and why a network changes over time. Network effects, like reciprocity and outdegree, only consider the ties between actors as important to network change, while actor effects allow for covariate values associated with each actor to influence ties between actors. In the SAOM literature, a network is denoted by $x(t_m)$, has $n$ actors, and for two actors $i \neq j \in \{1, 2, \dots, n\}$, the relationship between them is denoted by $x_ij$. This relationship $x_{ij}$ is equal to 1 if there is a tie between actors $i$ and $j$ and it is 0 if there is not a tie between them. The time component, $t_m$, represents the $m = 1, 2, \dots, M$ points of observation of the network. These networks $x(t_1), \dots, x(t_M)$ are modeled as observations of a continuous-time Markov chain, where most of the steps in the chain are unobserved. 

\par Between any two discrete time points, $t_{m}$ and $t_{m-1}$ for $m = 2, \dots, M$, each actor gets the opportunity to change one of their ties at a rate of $\alpha_{m-1}$. This is due to the properties of the Markov chain, and it also assumes that the rate of change is identical and constant for each actor between $t_{m-1}$ and $t_{m}$. When an actor is given an opportunity to change, it tries to maximize its objective function, $f_i(\beta, x)$, where $x$ is the potential new network state that actor $i$ can reach by changing one of its ties, $x_{ij}$. In model estimation, the parameters to be estimated are $\alpha_1, \dots, \alpha_{M-1}$ and the (possibly vector-valued) $\beta$. The objective function takes the form 

$$ f_i(\beta, x) = \sum_k \beta_k s_{ki}(x)$$ 

where $k = 1, \dots, K$, $K$ the number of non-rate parameters in the models and $s_{ki}(x)$ is the corresponding statistic. There are several parameters that can be chosen for use in the objective function. Examples of these parameters and their corresponding statistics are given in Table~\ref{tab:models}.

<<readsim, echo=FALSE, message=FALSE, warning=FALSE>>=
setwd("~/Desktop/NetworksResearch/NetworksVizInference/Results/")
nulls <- read.csv("../Data/distribution_null_model.csv")
# names(nulls)[-1] <- c("alpha1", "alpha2", "beta1", "beta2")
# nulls$Sim <- 1:nrow(nulls)
# nulls$Model <- "M1"
# 
alt <- read.csv("../Data/distribution_jumpTT_model.csv")
# names(alt)[-1] <- c("alpha1", "alpha2", "beta1", "beta2", "beta3")
# alt$Sim <- 1:nrow(alt)
# alt$Model <- "M2"
# 
alt2nd <- read.csv("../Data/distribution_dblpairs_model.csv")
library(dplyr)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
@

\begin{table}[h]
\centering
\scalebox{0.8}{
\begin{tabular}{lccrrr}
Effect name & Parameter & Corresponding Statistic & $M_1$  & $M_2$  & $M_3$ \\
\hline
\hline
Rate 1 (wave 1 $\rightarrow$ 2) & $\alpha_1$ & $\sum\limits_{i,j = 1 i\neq j}^n (x_{ij}(t_2) - x_{ij}(t_1))^2 $ & 
\Sexpr{round(null_mod_sv[1], 2)} &
\Sexpr{round(alt_mod_sv[1], 2)} & 
\Sexpr{round(alt_mod2_sv[1], 2)} 
\\
Rate 2 (wave 2 $\rightarrow$ 3) & $\alpha_2$ & $\sum\limits_{i,j = 1 i\neq j}^n (x_{ij}(t_3) - x_{ij}(t_2))^2 $ & 
\Sexpr{round(null_mod_sv[2], 2)} &
\Sexpr{round(alt_mod_sv[2], 2)} & 
\Sexpr{round(alt_mod2_sv[2], 2)}
\\
Outdegree & $\beta_1$ & $s_{i1}(x) = \sum\limits_{j=1}^n x_{ij}$ & 
\Sexpr{round(null_mod_sv[3], 2)} &
\Sexpr{round(alt_mod_sv[3], 2)} & 
\Sexpr{round(alt_mod2_sv[3], 2)}
\\
Reciprocity & $\beta_2$ & $s_{i2}(x) = \sum\limits_{j=1}^n x_{ij}x_{ji}$ & \Sexpr{round(null_mod_sv[4], 2)} &
\Sexpr{round(alt_mod_sv[4], 2)} & 
\Sexpr{round(alt_mod2_sv[4], 2)}
\\
Jumping Transitive Triplets & $\beta_3$ & $s_{i3}(x) = \sum\limits_{\forall j\neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(v_i = v_h \neq v_j)$ & -- & 
\Sexpr{round(alt_mod_sv[5], 2)} & -- \\
\# doubly achieved distances 2 effect & $\beta_4$ & $s_{i4}(x) = |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$ & -- & -- &
\Sexpr{round(alt_mod2_sv[5], 2)}
\end{tabular}}
\caption{\label{tab:models}Parameters and estimates of models $M_1$, $M_2$, and $M_3$. Estimates are the mean of 1000 iterations of the model estimates. The lineups that follow are simulated from models using these values.}
\end{table}

The outdegree and reciprocity effects are the most important effects to any SAOMs. The outdegree effect ``always must be included," and the reciprocity effect ``practically always must be included" {\color{green} (CITE Rsiena Manual, p 41)}. The outdegree of a node in a network is a measure of how active that node is within the network, while the reciprocity parameter measures of how often a node's outgoing ties are reciprocated.  The additional effects, jumping transitive triplets and doubly achieved distance, were deemed very significant by a $t$-test, which was why we chose them for testing. 


\begin{figure}
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:jtt}Realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node.}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)
library(geomnet)
set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, label = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, 
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{Doubly achieved distance between actors $i$ and $k$.}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, label = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}
\caption{\label{fig:structures}Structural newtwork effects. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$.}
\end{figure}

% BACKGROUND AND PURPOSE
The parameters in the objective function are tested for significance using $t$-tests. The test statistic is the ratio of the parameter estimate to its standard error. If this value is larger than 2 in absolute value, then the parameter is said to be significant at the $\alpha = 0.05$ level and should be included in the model. This is a fairly simple statistical test, so we wanted to test whether this significance can be detected visually just as simply as the statistical test detects it. If visualizations of simulated networks from two nested models have a much different appearance when placed side-by-side, then the difference in appearance can be attributed to the additional parameter in one. If, however, there is no visually detectable difference, then the additional parameter does not appear to have changed the network structure all that much. Because model selection and diagnostics for network models are less developed areas of the theory, testing network parameters in this visual way could lead to additional methods of model selection for networks. 

\section{Methods}
In order to test our hypothesis, we set up an amazon turk experiment using the lineup protocal of {\color{green} Buja et all (2009). FIX CITE}. We presented four types of lineups: M1 v. M2, M2 v. M1, M1 v. M3, and M3 v. M1, where M1, M2, and M3 have the objective functions givein in ~\ref{tab:models}. For each lineup, there were 12 plots shown: 11 of the plots were simulated from the first model (the ``null" model) and 1 was simulated from the second model (the ``alternative" model). The order of the plots in the lineups was randomly assigned. 

In order to become a subject in our experiment, the Amazon turk user had to first read through some introductory material and prove they could identify the correct plot in two test lineups, one for M1 v. M2 and one for M2 v. M1. These two lineups were constructed to be very simple in order to train the turkers. Once the turker made it into the experiment, they looked at 10 lineups selected at random from a pool of 25. There were five lineups for each of the four types with an additional five lineups of the type M1 v. M2 which were chosen for their high counts of jumping transitive triplets in the alternative model network in an attempt to gauge how important this statistic is to the visualization of the network. If there are many jumping transitive triplets in the alternative model plot and more turkers correctly choose that plot, that would be evidence that the jumping transitive triplets are very noticeable to the user. \st{????}

\section{Procedure}

Each Amazon Turk user was greeted with the following message: ``In this survey a series of similar looking charts will be presented. We would like you to respond to the following questions.
\begin{enumerate}
\item Pick the plot based on the survey question
\item Provide reasons for choice
\item How certain are you?
\end{enumerate}
Finally we would like to collect some information about you. (age category, education and gender)."  Then, they were led through a training page about how to identify the correct plot in a lineup, seen in Figure~\ref{fig:lineupex}. Then, they saw two trial plots that they had to get correct in order to proceed to the rest of the experiment. They chose the plot that looked the most different from the others, why they chose is (most complex, least complex, or other), and how certain they were that they had chosen correctly (Very Uncertain, Uncertain, Neutral, Certain, Very Certain). These trial plots are shown in Figure~\ref{fig:lineuptrial} 

\begin{figure}
\begin{subfigure}[t]{.45\textwidth}
\caption{Example 1}
\includegraphics[width=\textwidth]{Ex1}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{Example 2}
\includegraphics[width=\textwidth]{Ex2}
\end{subfigure}
\caption{\label{fig:lineupex} The first training page in the Amazon Turk experiment.}
\end{figure}

\begin{figure}
\begin{subfigure}[t]{.45\textwidth}
\caption{Trial 1}
\includegraphics[width=\textwidth]{Trial1}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{Trial 2}
\includegraphics[width=\textwidth]{Trial2}
\end{subfigure}
\caption{\label{fig:lineuptrial} The trial plots that users had to get correct in order to participate in the Amazon Turk experiment.}
\end{figure}

Once the turk user chose the correct plot in the two trial plots, the experiment proceeded with the same interface and users selecting which plot they thought was most different, why they thought that, and how certain they were for 10 plots chosen at random. 

\section{Results}

<<get_res, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.cap="Boxplot showing the percent of users correctly choosing the alternative model in all 25 lineups. In 15 out of 25 lineups, no user picked the correct plot. The maximum was 64.00\\%, and the second highest was 35.29\\% correct.", fig.height=3>>=
ids <- readr::read_csv("https://raw.githubusercontent.com/erichare/lineups/f19635a00507210013ba9e631761ad0c89a6564d/experiments/turk21/details/picture-details.csv")
tab <- readr::read_csv("../Results/turk21_users.csv")
tab2 <- unique(tab)
#length(unique(tab2$nick_name))
#head(sort(table(tab2$nick_name),decreasing = T))
#tab2 %>% filter(nick_name == "A3U3L1XQVR362A")
#tab2 %>% filter(nick_name == "ADTIO3A6TM9CG")
#tab2 %>% filter(nick_name %in% names(table(res$nick_name))[table(res$nick_name) != 10])
res <- readr::read_csv("../Results/turk21_feedback.csv")
# remove people who only completed 1 plot
res <- res %>% filter(!(nick_name %in% names(table(res$nick_name))[table(res$nick_name) != 10]))
#intersect(names(ids), names(res))
res2 <- left_join(res, ids, by = 'pic_id')
res2$correct <- res2$response_no == res2$obs_plot_location
res2$time <- res2$end_time - res2$start_time

res2$lineup_name <- as.factor(paste(res2$test_param, res2$param_value))
levels(res2$lineup_name) <- c(paste("M1 v M2 rep", 1:10), paste("M1 v M3 rep", 1:5),
                              paste("M3 v M1 rep", 1:5), paste("M2 v M1 rep", 1:5))


res2 %>% group_by(lineup_name) %>% 
  summarize(total = n(), tot_correct = sum(correct), perc_correct = tot_correct/total) %>% 
  arrange(desc(perc_correct)) -> lineup_by_perc_correct
#lineup_by_perc_correct[which.max(lineup_by_perc_correct$total),]
#lineup_by_perc_correct[which.min(lineup_by_perc_correct$total),]
# box plot
ggplot(data = lineup_by_perc_correct, aes(x = 1, y = perc_correct)) + geom_boxplot() + 
  scale_y_continuous(breaks = seq(0, 1, .10), labels = paste0(seq(0, 1, .10)*100, "%")) + 
  coord_flip() + theme_classic() + 
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
  labs(x = "", title = "Percent of Users Correctly Identifying Alternate Model", y = "")
@

We collected 10 lineups each from 77 Amazon Turk users on July 28, 2016 from 19:11:03 GMT to 22:17:47 GMT. A table of demographic information collected on the participants in the experiment is in Table~\ref{fig:demos}. 

\begin{figure}
\begin{subfigure}[c]{.45\textwidth}
\textbf{Gender:} \\
<<tab_demo, echo=FALSE, results='asis'>>=
t1 <- t(data.frame(table(tab2$gender)))
rownames(t1) <- c("Gender:", "Count:")
colnames(t1) <- t1[1,]
t1 <- t1[2,]
kable(t1, format = 'latex')
@
\vspace{.5cm}
\textbf{Education Level:} \\
<<tab_demo3, echo=FALSE, results='asis'>>=
t3 <- t(data.frame(table(tab2$academic_study)))
rownames(t3) <- c("Highest Education:", "Count:")
colnames(t3) <- t3[1,]
t3 <- t3[2,]
kable(t3, format = 'latex')
@
\end{subfigure}\hfill
\begin{subfigure}[c]{.45\textwidth}
\centering
\textbf{Age Group:} \\
<<tab_demo2, echo=FALSE, results='asis'>>=
t2 <- t(data.frame(table(tab2$age)))
rownames(t2) <- c("Age:", "Count:")
colnames(t2) <- t2[1,]
t2 <- t2[2,]
kable(t2, format = 'latex')
@
\end{subfigure}
\caption{\label{fig:demos} Demographic information on the Amazon Turk users participating in the experiment}
\end{figure}


Because of the random assignment of the 25 plots to users, almost every lineup was seen by a different number of people. Repetition 3 of M1 v. M3 was seen by 47 different users while repetition 2 of the same type was seen by only 6 different users. The mean number of times seens was 30.8 and the median was 31. In 15 of the 25 lineups, no user chose the correct plot, and in the remaining 10 lineups, the users correctly identified the alternative plot a minimum of 4.35\% of the time and a maximum of 64\% of the time. A plot showing the number of different plots chosen, how many times they were chosen, and whether or not it was the correct choice is given in Figure~\ref{fig:facets_correct}. 

<<facets_correct, echo=FALSE, fig.align="center", fig.cap="Plots chosen for each of the 25 lineups in the experiment. The x-axis ticks do not have a label because the label is less important than the number of different plots users chose and whether or not they chose the correct plot, shown in the coloring of the bars.", fig.height=9>>=
ggplot(data = res2, aes(x = as.factor(response_no), fill = correct)) + 
  geom_bar(color = 'black') + 
  scale_fill_manual(values = c("white", "grey30"), name = "Correct?") + 
  facet_wrap(~lineup_name, scales = "free", nrow = 5) +
  theme_bw() + 
  theme(axis.text.x = element_blank(), legend.position = 'bottom') + 
  labs(x = "User Response" , y = "Number of Responses")
@

This plot shows us the abysmal ability of the Turkers to identify the alternative plot correctly. There are also a few other interesting results from this plot. First, there are several lineups where a majority of participants selected the same wrong alternative plot. Additionally, we see a lot of variation in the number of different plots selected at the alternate plot by the Turkers. At most, there were 10 different plots selected, compared to 2 at the opposite end. \st{XXX is it worth exploring these ``interesting things" more in-depth? XXX}

Next, we investigate the confidence of the experiment's participants in selecting the most different plot from the others. Overall, in 40\% of the responses, the respondent was certain they were correct. User confidence in the remaining categories, neutral, uncertain, very certain, and very uncertain, was 26.88\%, 16.88\%, 9.48\%, and 6.76\%, respectively. These results are summarized by lineup in Figure~\ref{fig:cert_plot}. We also performed a 5-sample test for equality of proportions in order to test the null hypothesis that the proportion of correct responses is the same in all 5 certainty categories. This test resulted in a $p$-value of 0.5881, so there is no evidence that the certainty of a user's response affects whether or not they choose the correct plot. This provides further evidence that detecting a network simulated from a different model is an extremely difficult problem. 

<<cert_plot, echo=FALSE, fig.align='center', fig.height=4, fig.cap="All responses for all lineups, separated by whether the plot selected was the true alternative plot (TRUE) or not (FALSE) and colored by the respondent's uncertainty levels. We see here that most respondents were certain in their answer whether or not they were correct. ">>=
res2$conf_level <- ordered(res2$conf_level, levels = c("Very Uncertain", "Uncertain", "Neutral", "Certain", "Very Certain"))

ggplot(data = res2, aes(x = lineup_name)) + 
  geom_bar(aes(fill = conf_level), color = 'grey60') + 
  scale_fill_brewer(palette = "RdBu") + 
  coord_flip() + 
  facet_wrap(~correct) + 
  theme_bw() + 
  theme(legend.position = 'bottom')

summary.conf <- data.frame(table(res2$conf_level, res2$correct))
summary.conf %>% tidyr::spread(Var2, Freq) -> summary.conf
rownames(summary.conf) <- as.character(summary.conf[,1])
names(summary.conf) <- c("Var1", "Failures", "Successes")
summary.conf <- summary.conf[,-1]
summary.conf <- summary.conf[,c(2,1)]
prop_test_for_conf_corr <- prop.test(as.matrix(summary.conf))
@

We next investigate the length of time that users' took to respond to each lineup. The minimum was 3.762 seconds and the maximum was 578.8 seconds (nearly 10 minutes). The median was 13.01 seconds and the mean was 20.32 seconds. First, a 2-sample Kolmogorov-Smirnov test for equality of distribution was done to see if the distribution of times for correct plots chosen is the same as the distribution of times for incorrect plots chosen. This two-sided test resulted in a $p$-value of 1, so there is no evidence that the distribution of times is different whether or not the response was correct. 

<<time_plot, echo=FALSE>>=
#qplot(x = res2$time, binwidth = 2)
ggplot(data = res2, aes(x = conf_level, y = time)) + geom_boxplot() + coord_flip()
ggplot(data = res2, aes(x = correct, y = time)) + geom_boxplot() + coord_flip() + theme_bw()
ggplot(data = res2, aes(x = time)) + geom_histogram(binwidth = 5, fill ='white', color = 'black') + facet_wrap(~correct, nrow = 2)
ggplot(data = res2, aes(x = lineup_name, y = time)) + 
  geom_boxplot() + 
#  facet_wrap(~conf_level) + 
  coord_flip() + 
  theme_bw() + 
  labs(x = "Time (in seconds)", y = "Lineup Name")
# 2-sample kolmogorov-smirnov test for equality of distributions
# ks.test(x = summary(res2$time[res2$correct]), y = summary(res2$time[!res2$correct]))
# fail to reject null
@

\end{document}